{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexeyK12/Implementation-of-the-article-StyleGAN-NADA/blob/main/app_StyleGAN_NADA_AK_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#########################\n",
        "# УСТАНОВКА ЗАВИСИМОСТЕЙ\n",
        "#########################\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "#!pip uninstall google-genai -y\n",
        "#!pip install gradio\n",
        "\n",
        "import os\n",
        "os.environ[\"TORCH_CUDA_ARCH_LIST\"] = \"7.5\"\n",
        "\n",
        "!apt-get -y install ninja-build\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install gradio==3.39.0\n",
        "\n",
        "if not os.path.exists(\"Article-implementation-StyleGAN-NADA\"):\n",
        "    !git clone https://github.com/AlexeyK12/Article-implementation-StyleGAN-NADA.git\n",
        "\n",
        "if not os.path.exists(\"stylegan2-ffhq-config-f.pt\"):\n",
        "    !wget https://huggingface.co/akhaliq/OneshotCLIP-stylegan2-ffhq/resolve/main/stylegan2-ffhq-config-f.pt -O stylegan2-ffhq-config-f.pt\n",
        "\n",
        "%cd Article-implementation-StyleGAN-NADA\n",
        "\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "\n",
        "from model import Generator\n",
        "import clip\n",
        "\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "outdir = \"/content/outputs_nada_colab\"\n",
        "os.makedirs(outdir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################\n",
        "# ИНИЦИАЛИЗАЦИЯ ФУНКЦИЙ\n",
        "########################\n",
        "\n",
        "# загрузка CLIP\n",
        "clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
        "clip_model.eval().requires_grad_(False)\n",
        "\n",
        "# загрузка StyleGAN2\n",
        "ckpt_path_base = \"/content/stylegan2-ffhq-config-f.pt\"\n",
        "\n",
        "latent_dim = 512\n",
        "image_size = 1024\n",
        "\n",
        "base_generator = Generator(\n",
        "    size=image_size,\n",
        "    style_dim=latent_dim,\n",
        "    n_mlp=8,\n",
        "    channel_multiplier=2\n",
        ").to(device)\n",
        "\n",
        "ckpt = torch.load(ckpt_path_base, map_location=device)\n",
        "base_generator.load_state_dict(ckpt[\"g_ema\"], strict=False)\n",
        "\n",
        "def copy_generator(src_gen):\n",
        "    import copy\n",
        "    new_gen = Generator(\n",
        "    size=image_size,\n",
        "    style_dim=latent_dim,\n",
        "    n_mlp=8,\n",
        "    channel_multiplier=2\n",
        "    ).to(device)\n",
        "\n",
        "    new_gen.load_state_dict(src_gen.state_dict(), strict=False)\n",
        "    return new_gen\n",
        "\n",
        "# заморозка coarse-слоёв\n",
        "def freeze_layers(g, freeze_num=2):\n",
        "    blocks_to_freeze = freeze_num * 2\n",
        "    for i, conv_block in enumerate(g.convs):\n",
        "        if i < blocks_to_freeze:\n",
        "            for param in conv_block.parameters():\n",
        "                param.requires_grad = False\n",
        "    for i, to_rgb_layer in enumerate(g.to_rgbs):\n",
        "        if i < freeze_num:\n",
        "            for param in to_rgb_layer.parameters():\n",
        "                param.requires_grad = False\n",
        "    return g\n",
        "\n",
        "# предобработка для CLIP\n",
        "def clip_preprocess_tensor(imgs):\n",
        "    imgs = (imgs.clamp(-1,1) + 1)/2\n",
        "    imgs_224 = F.interpolate(imgs, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "    clip_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073], device=device).view(1, -1, 1, 1)\n",
        "    clip_std  = torch.tensor([0.26862954, 0.26130258, 0.27577711], device=device).view(1, -1, 1, 1)\n",
        "    imgs_224 = (imgs_224 - clip_mean) / clip_std\n",
        "    return imgs_224\n",
        "\n",
        "# сохранение модели\n",
        "def save_checkpoint(gen, path):\n",
        "    torch.save({\"g_ema\": gen.state_dict()}, path)\n",
        "\n",
        "# функция обучения\n",
        "def train_model(\n",
        "    freeze_until=1,\n",
        "    steps=500,\n",
        "    batch_size=2,\n",
        "    lr=1e-4,\n",
        "    prompt_target=\"joker\"\n",
        "):\n",
        "    generator = copy_generator(base_generator)\n",
        "    generator.train()\n",
        "\n",
        "    # замораживаем слои\n",
        "    generator = freeze_layers(generator, freeze_num=freeze_until)\n",
        "\n",
        "    # текстовые эмбеддинги\n",
        "    prompt_orig = \"face\"\n",
        "    text_orig_emb = clip_model.encode_text(clip.tokenize(prompt_orig).to(device))\n",
        "    text_target_emb = clip_model.encode_text(clip.tokenize(prompt_target).to(device))\n",
        "    text_orig_emb   = text_orig_emb / text_orig_emb.norm(dim=-1, keepdim=True)\n",
        "    text_target_emb = text_target_emb / text_target_emb.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    params_to_optimize = [p for p in generator.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.Adam(params_to_optimize, lr=lr, betas=(0.0, 0.99))\n",
        "\n",
        "    alpha = 0.5\n",
        "\n",
        "    # цикл обучения\n",
        "    pbar = range(steps)\n",
        "    for step_i in pbar:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        z = torch.randn(batch_size, latent_dim, device=device)\n",
        "        generated_imgs, _ = generator([z], truncation=1, input_is_latent=False)\n",
        "\n",
        "        clip_in = clip_preprocess_tensor(generated_imgs)\n",
        "        image_embeds = clip_model.encode_image(clip_in)\n",
        "        image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        target_sim = (image_embeds * text_target_emb).sum(dim=-1)\n",
        "        orig_sim   = (image_embeds * text_orig_emb).sum(dim=-1)\n",
        "\n",
        "        clip_loss = - (target_sim - alpha * orig_sim).mean()\n",
        "        clip_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        yield f\"Шаг {step_i+1}/{steps} | Текущая clip_loss = {clip_loss.item():.4f}\"\n",
        "\n",
        "    final_ckpt_path = os.path.join(outdir, \"stylegan2_nada_final.pt\")\n",
        "    save_checkpoint(generator, final_ckpt_path)\n",
        "\n",
        "    yield f\"Обучение завершено!\\nФинальный чекпоинт сохранён по пути: {final_ckpt_path}\"\n",
        "\n",
        "# функция инференса\n",
        "def infer_model():\n",
        "    final_ckpt_path = os.path.join(outdir, \"stylegan2_nada_final.pt\")\n",
        "    if not os.path.exists(final_ckpt_path):\n",
        "        return [Image.new(\"RGB\", (256, 256), color=(180, 0, 0))]\n",
        "\n",
        "    gen_for_infer = copy_generator(base_generator)\n",
        "    checkpoint = torch.load(final_ckpt_path, map_location=device)\n",
        "    gen_for_infer.load_state_dict(checkpoint[\"g_ema\"], strict=False)\n",
        "    gen_for_infer.eval()\n",
        "\n",
        "    result_images = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(5):\n",
        "            z = torch.randn(1, latent_dim, device=device)\n",
        "            img, _ = gen_for_infer([z], truncation=1, input_is_latent=False)\n",
        "            img = (img.clamp(-1, 1) + 1) / 2\n",
        "            img = (img * 255).permute(0, 2, 3, 1).cpu().numpy().astype(np.uint8)[0]\n",
        "            pil_img = Image.fromarray(img)\n",
        "\n",
        "            scale_factor = 1/3\n",
        "            w, h = pil_img.size\n",
        "            new_w, new_h = int(w*scale_factor), int(h*scale_factor)\n",
        "            pil_img_small = pil_img.resize((new_w, new_h), Image.LANCZOS)\n",
        "\n",
        "            result_images.append(pil_img_small)\n",
        "    return result_images\n",
        "\n",
        "\n",
        "# инверсия\n",
        "def invert_image(img: Image.Image, generator, steps=300, lr=0.01):\n",
        "    generator.eval()\n",
        "\n",
        "    img_t = torch.from_numpy(np.array(img)).float().to(device) / 255.0\n",
        "    if len(img_t.shape) == 3:\n",
        "        img_t = img_t.permute(2, 0, 1).unsqueeze(0)\n",
        "    img_t = img_t * 2 - 1\n",
        "    img_t = F.interpolate(img_t, size=(1024, 1024), mode='bilinear', align_corners=False)\n",
        "\n",
        "    z_opt = torch.randn(1, latent_dim, device=device, requires_grad=True)\n",
        "\n",
        "    optimizer = torch.optim.Adam([z_opt], lr=lr)\n",
        "\n",
        "    for step in range(steps):\n",
        "        optimizer.zero_grad()\n",
        "        gen_img, _ = generator([z_opt], truncation=1, input_is_latent=False)\n",
        "\n",
        "        loss = F.mse_loss(gen_img, img_t)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return z_opt.detach()\n",
        "\n",
        "\n",
        "def transform_image(input_img: Image.Image):\n",
        "    try:\n",
        "        outdir = \"/content/outputs_nada_colab\"\n",
        "        final_ckpt_path = os.path.join(outdir, \"stylegan2_nada_final.pt\")\n",
        "        if not os.path.exists(final_ckpt_path):\n",
        "            return Image.new(\"RGB\", (256, 256), color=(255, 0, 0))\n",
        "\n",
        "        gen_for_infer = copy_generator(base_generator)\n",
        "        checkpoint = torch.load(final_ckpt_path, map_location=device)\n",
        "        gen_for_infer.load_state_dict(checkpoint[\"g_ema\"], strict=False)\n",
        "        gen_for_infer.to(device).eval()\n",
        "\n",
        "        z_approx = invert_image(input_img, gen_for_infer, steps=300, lr=0.01)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            gen_img, _ = gen_for_infer([z_approx], truncation=1, input_is_latent=False)\n",
        "\n",
        "        gen_img = (gen_img.clamp(-1, 1) + 1) * 0.5\n",
        "        gen_img = gen_img.mul(255).byte().permute(0, 2, 3, 1).cpu().numpy()\n",
        "        pil_img = Image.fromarray(gen_img[0], 'RGB')\n",
        "\n",
        "        scale_factor = 1/4\n",
        "        w, h = pil_img.size\n",
        "        pil_img_small = pil_img.resize((int(w*scale_factor), int(h*scale_factor)), Image.LANCZOS)\n",
        "\n",
        "        return pil_img_small\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Ошибка внутри transform_image:\", str(e))\n",
        "        raise e\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "####################\n",
        "# ПРИЛОЖЕНИЕ GRADIO\n",
        "####################\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# App StyleGAN-NADA by AK_12\")\n",
        "\n",
        "    gr.Image(\n",
        "        value=\"/content/Article-implementation-StyleGAN-NADA/DALL·E-hum-joker.png\",\n",
        "        interactive=False,\n",
        "        label=\"\",\n",
        "        width=512,\n",
        "        height=512\n",
        "    )\n",
        "\n",
        "    with gr.Tab(\"Обучение\"):\n",
        "        gr.Markdown(\"Параметры обучения:\")\n",
        "        freeze_slider   = gr.Slider(minimum=0, maximum=5, step=1, value=1, label=\"freeze_until\")\n",
        "        steps_slider    = gr.Slider(minimum=100, maximum=2000, step=50, value=500, label=\"steps\")\n",
        "        batch_slider    = gr.Slider(minimum=1, maximum=8, step=1, value=2, label=\"batch_size\")\n",
        "        lr_number       = gr.Number(value=1e-4, label=\"learning rate (lr)\")\n",
        "        prompt_target_t = gr.Textbox(value=\"joker\", label=\"prompt_target\")\n",
        "\n",
        "        train_btn = gr.Button(\"Запустить обучение\")\n",
        "        train_output = gr.Textbox(label=\"Лог обучения\")\n",
        "\n",
        "        train_btn.click(\n",
        "            fn=train_model,\n",
        "            inputs=[freeze_slider, steps_slider, batch_slider, lr_number, prompt_target_t],\n",
        "            outputs=train_output\n",
        "        )\n",
        "\n",
        "    with gr.Tab(\"Инференс\"):\n",
        "        gr.Markdown(\"Генерация 5 изображений с обученной моделью по промту\")\n",
        "        infer_btn = gr.Button(\"Генерировать\")\n",
        "        gallery = gr.Gallery(label=\"Сгенерированные изображения\", show_label=False).style(grid=[5], height=\"auto\")\n",
        "\n",
        "        infer_btn.click(\n",
        "            fn=infer_model,\n",
        "            inputs=[],\n",
        "            outputs=gallery\n",
        "        )\n",
        "\n",
        "    with gr.Tab(\"Image2Image\"):\n",
        "        gr.Markdown(\"Загрузите свою картинку, и модель попытается её стилизовать.\")\n",
        "        input_image = gr.Image(label=\"Ваше изображение\")\n",
        "        stylize_btn = gr.Button(\"Stylize\")\n",
        "        output_image = gr.Image(label=\"Результат\", interactive=False)\n",
        "\n",
        "        stylize_btn.click(\n",
        "            fn=transform_image,\n",
        "            inputs=[input_image],\n",
        "            outputs=[output_image]\n",
        "        )\n",
        "\n",
        "demo.queue()\n",
        "app_url = demo.launch(share=True, debug=True)\n",
        "\n",
        "from IPython.display import Javascript\n",
        "Javascript(f'window.open(\"{app_url}\");')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S4e-siwDok3Z",
        "outputId": "100cac9e-367e-4f4a-ca03-87df0cbf6871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Feb  1 11:02:35 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P0             28W /   70W |    1660MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ninja-build is already the newest version (1.10.1-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-pbrjm7gf\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-pbrjm7gf\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.20.1+cu124)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
            "Requirement already satisfied: gradio==3.39.0 in /usr/local/lib/python3.11/dist-packages (3.39.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (23.2.1)\n",
            "Requirement already satisfied: aiohttp~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (3.11.11)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (5.5.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (0.115.8)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (0.5.0)\n",
            "Requirement already satisfied: gradio-client>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (1.7.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (0.27.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (3.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.39.0) (2.2.0)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (3.10.0)\n",
            "Requirement already satisfied: mdit-py-plugins<=0.3.3 in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (0.3.3)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (2.2.2)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (10.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (2.10.6)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (0.25.1)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (6.0.2)\n",
            "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (2.32.3)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (2.10.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (0.34.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (11.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.0->gradio==3.39.0) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.0->gradio==3.39.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.0->gradio==3.39.0) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.0->gradio==3.39.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.0->gradio==3.39.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.0->gradio==3.39.0) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.0->gradio==3.39.0) (1.18.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6.0,>=4.2.0->gradio==3.39.0) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6.0,>=4.2.0->gradio==3.39.0) (1.24.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client>=0.3.0->gradio==3.39.0) (2024.10.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->gradio==3.39.0) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->gradio==3.39.0) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->gradio==3.39.0) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->gradio==3.39.0) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->gradio==3.39.0) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.14.0->gradio==3.39.0) (3.17.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.14.0->gradio==3.39.0) (4.67.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.0.0->markdown-it-py[linkify]>=2.0.0->gradio==3.39.0) (0.1.2)\n",
            "Requirement already satisfied: linkify-it-py<3,>=1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.39.0) (2.0.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==3.39.0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==3.39.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==3.39.0) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==3.39.0) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==3.39.0) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==3.39.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio==3.39.0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio==3.39.0) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.39.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.39.0) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.0->gradio==3.39.0) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.0->gradio==3.39.0) (2.3.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.14.0->gradio==3.39.0) (8.1.8)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi->gradio==3.39.0) (0.45.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.39.0) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.39.0) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.39.0) (0.22.3)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.11/dist-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio==3.39.0) (1.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio==3.39.0) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->gradio==3.39.0) (1.3.1)\n",
            "Cloning into 'Article-implementation-StyleGAN-NADA'...\n",
            "remote: Enumerating objects: 147, done.\u001b[K\n",
            "remote: Counting objects: 100% (147/147), done.\u001b[K\n",
            "remote: Compressing objects: 100% (141/141), done.\u001b[K\n",
            "remote: Total 147 (delta 73), reused 30 (delta 4), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (147/147), 14.10 MiB | 19.73 MiB/s, done.\n",
            "Resolving deltas: 100% (73/73), done.\n",
            "--2025-02-01 11:02:51--  https://huggingface.co/akhaliq/OneshotCLIP-stylegan2-ffhq/resolve/main/stylegan2-ffhq-config-f.pt\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.23, 18.164.174.55, 18.164.174.17, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.23|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.hf.co/repos/e9/a3/e9a3ec3ca794fe184757f39017bb3836044f5de0c2ff76d7109137c040c4b817/bae494ef77e32a9cd1792a81a3c167692a0e64f6bcd8b06592ff42917e2ed46e?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27stylegan2-ffhq-config-f.pt%3B+filename%3D%22stylegan2-ffhq-config-f.pt%22%3B&Expires=1738410941&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczODQxMDk0MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9lOS9hMy9lOWEzZWMzY2E3OTRmZTE4NDc1N2YzOTAxN2JiMzgzNjA0NGY1ZGUwYzJmZjc2ZDcxMDkxMzdjMDQwYzRiODE3L2JhZTQ5NGVmNzdlMzJhOWNkMTc5MmE4MWEzYzE2NzY5MmEwZTY0ZjZiY2Q4YjA2NTkyZmY0MjkxN2UyZWQ0NmU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=UDnUh3pqfaNrbSm-orC8rhkMI6XS4ujV5nFqevPC-9JnEk3XonA7AFhhTMH%7E8FnMtOwEWg4R%7EOgHFlgsjLKRSaHCFjw9qHXiTVFYJICme7ZQV4r2WQHBJimxsBvZSqDwit-pfhNGySePhvmy8OgyPx0KHW1EuElVRcT7rgo1iu5I1hstXVtesVJBXBNKzJPDfA7CX9bzhpu7afZSiYYhnDn%7EKwISe2loOT-1mUXaad0jfXDW%7EiyNsAjEsSHS91TPM8ShJvl9ftu4cM08A1-a0HdiJ66ZTKQDEDaOnqkied1mJQjIcQYND7rVKAIEcjTKExWRlC9N47xR0AH7Q63uGQ__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
            "--2025-02-01 11:02:51--  https://cdn-lfs.hf.co/repos/e9/a3/e9a3ec3ca794fe184757f39017bb3836044f5de0c2ff76d7109137c040c4b817/bae494ef77e32a9cd1792a81a3c167692a0e64f6bcd8b06592ff42917e2ed46e?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27stylegan2-ffhq-config-f.pt%3B+filename%3D%22stylegan2-ffhq-config-f.pt%22%3B&Expires=1738410941&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczODQxMDk0MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9lOS9hMy9lOWEzZWMzY2E3OTRmZTE4NDc1N2YzOTAxN2JiMzgzNjA0NGY1ZGUwYzJmZjc2ZDcxMDkxMzdjMDQwYzRiODE3L2JhZTQ5NGVmNzdlMzJhOWNkMTc5MmE4MWEzYzE2NzY5MmEwZTY0ZjZiY2Q4YjA2NTkyZmY0MjkxN2UyZWQ0NmU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=UDnUh3pqfaNrbSm-orC8rhkMI6XS4ujV5nFqevPC-9JnEk3XonA7AFhhTMH%7E8FnMtOwEWg4R%7EOgHFlgsjLKRSaHCFjw9qHXiTVFYJICme7ZQV4r2WQHBJimxsBvZSqDwit-pfhNGySePhvmy8OgyPx0KHW1EuElVRcT7rgo1iu5I1hstXVtesVJBXBNKzJPDfA7CX9bzhpu7afZSiYYhnDn%7EKwISe2loOT-1mUXaad0jfXDW%7EiyNsAjEsSHS91TPM8ShJvl9ftu4cM08A1-a0HdiJ66ZTKQDEDaOnqkied1mJQjIcQYND7rVKAIEcjTKExWRlC9N47xR0AH7Q63uGQ__&Key-Pair-Id=K3RPWS32NSSJCE\n",
            "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 3.169.231.4, 3.169.231.87, 3.169.231.115, ...\n",
            "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|3.169.231.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 381462551 (364M) [binary/octet-stream]\n",
            "Saving to: ‘stylegan2-ffhq-config-f.pt’\n",
            "\n",
            "stylegan2-ffhq-conf 100%[===================>] 363.79M  74.8MB/s    in 4.9s    \n",
            "\n",
            "2025-02-01 11:02:56 (75.0 MB/s) - ‘stylegan2-ffhq-config-f.pt’ saved [381462551/381462551]\n",
            "\n",
            "/content/Article-implementation-StyleGAN-NADA/Article-implementation-StyleGAN-NADA/Article-implementation-StyleGAN-NADA/Article-implementation-StyleGAN-NADA/Article-implementation-StyleGAN-NADA/Article-implementation-StyleGAN-NADA/Article-implementation-StyleGAN-NADA\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-ea24632a26ad>:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(ckpt_path_base, map_location=device)\n",
            "<ipython-input-7-ea24632a26ad>:286: GradioDeprecationWarning: The `style` method is deprecated. Please set these arguments in the constructor instead.\n",
            "  gallery = gr.Gallery(label=\"Сгенерированные изображения\", show_label=False).style(grid=[5], height=\"auto\")\n",
            "<ipython-input-7-ea24632a26ad>:286: GradioDeprecationWarning: The 'grid' parameter will be deprecated. Please use 'grid_cols' in the constructor instead.\n",
            "  gallery = gr.Gallery(label=\"Сгенерированные изображения\", show_label=False).style(grid=[5], height=\"auto\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMPORTANT: You are using gradio version 3.39.0, however version 4.44.1 is available, please upgrade.\n",
            "--------\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://02949ac066c5f89920.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://02949ac066c5f89920.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/Article-implementation-StyleGAN-NADA/op/conv2d_gradfix.py:88: UserWarning: conv2d_gradfix not supported on PyTorch 2.5.1+cu124. Falling back to torch.nn.functional.conv2d().\n",
            "  warnings.warn(\n",
            "<ipython-input-7-ea24632a26ad>:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(final_ckpt_path, map_location=device)\n"
          ]
        }
      ]
    }
  ]
}